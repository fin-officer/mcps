# Uniwersalny Serwer Ollama

![GUI.png](GUI.png)

Kompletne, uniwersalne rozwiÄ…zanie do uruchamiania i zarzÄ…dzania lokalnym serwerem modeli jÄ™zykowych poprzez Ollama. DziaÅ‚a na wszystkich dystrybucjach Linuxa oraz macOS.

## Funkcje

- ğŸš€ **UniwersalnoÅ›Ä‡** - dziaÅ‚a na wszystkich dystrybucjach Linux oraz macOS
- ğŸ”„ **Automatyczna konfiguracja** - automatyczna instalacja wymaganych pakietÃ³w
- ğŸ“ **Interfejs webowy** - do testowania modeli w przeglÄ…darce
- âš™ï¸ **Åatwa konfiguracja** - przez plik .env lub parametry wiersza poleceÅ„
- ğŸŒ **Proste API REST** - do integracji z aplikacjami
- ğŸ”„ **ObsÅ‚uga wielu modeli** - z moÅ¼liwoÅ›ciÄ… Å‚atwego przeÅ‚Ä…czania
- ğŸ“Š **Zaawansowane zarzÄ…dzanie parametrami** - temperatura, max_tokens, itp.

## Wymagania

- Python 3.8+ (skrypt wykryje i zainstaluje zaleÅ¼noÅ›ci)
- [Ollama](https://ollama.com/download)
- Co najmniej jeden model Ollama (np. tinyllama, llama3, qwen, phi)

## Szybki start

1. **Pobierz i utwÃ³rz skrypt:**

   ```bash
   # Zapisz skrypt jako setup.sh
   chmod +x setup.sh
   ```

2. **Uruchom z peÅ‚nÄ… konfiguracjÄ…:**

   ```bash
   ./setup.sh
   ```

   Skrypt automatycznie:
   - Sprawdzi i zainstaluje wymagane pakiety Python
   - Sprawdzi instalacjÄ™ Ollama
   - PomoÅ¼e w konfiguracji modeli
   - Uruchomi serwer API

3. **Lub tylko uruchom serwer (jeÅ›li juÅ¼ skonfigurowany):**

   ```bash
   ./setup.sh --run
   ```

## DostÄ™pne opcje

Skrypt oferuje kilka przydatnych opcji:

```bash
# WyÅ›wietlenie pomocy
./setup.sh --help

# Tylko konfiguracja Å›rodowiska (bez uruchamiania)
./setup.sh --setup

# Tylko uruchomienie serwera
./setup.sh --run

# Uruchomienie na niestandardowym porcie
./setup.sh --run --port 8080

# ZarzÄ…dzanie modelami Ollama
./setup.sh --models

# Sprawdzenie wymagaÅ„ systemowych
./setup.sh --check
```

## Interfejs webowy

Po uruchomieniu serwera, interfejs webowy jest dostÄ™pny pod adresem:

```
http://localhost:5001
```

Interfejs pozwala na:
- Zadawanie pytaÅ„ do modelu
- ZmianÄ™ parametrÃ³w generowania
- PrzeglÄ…danie dostÄ™pnych modeli

## API REST

Serwer udostÄ™pnia nastÄ™pujÄ…ce endpointy:

### `POST /ask`

WysyÅ‚a zapytanie do modelu Ollama.

**Å»Ä…danie**:
```json
{
  "prompt": "Co to jest Python?",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**OdpowiedÅº**:
```json
{
  "response": "Python to wysokopoziomowy, interpretowany jÄ™zyk programowania..."
}
```

### `GET /models`

Pobiera listÄ™ dostÄ™pnych modeli Ollama.

**OdpowiedÅº**:
```json
{
  "models": [
    {
      "name": "tinyllama:latest",
      "size": 1640,
      "current": true
    },
    {
      "name": "llama3:latest",
      "size": 3827,
      "current": false
    }
  ]
}
```

### `POST /echo`

Proste narzÄ™dzie do testowania dziaÅ‚ania serwera.
![img.png](img.png)
**Å»Ä…danie**:
```json
{
  "message": "Test"
}
```

**OdpowiedÅº**:
```json
{
  "response": "Otrzymano: Test"
}
```

## UÅ¼ywanie z cURL

```bash
# Zapytanie do modelu
curl -X POST -H "Content-Type: application/json" \
     -d '{"prompt":"Co to jest Python?"}' \
     http://localhost:5001/ask

# Pobranie listy modeli
curl http://localhost:5001/models

# Test echo
curl -X POST -H "Content-Type: application/json" \
     -d '{"message":"Test"}' \
     http://localhost:5001/echo
```

## Plik konfiguracyjny .env

Skrypt tworzy plik `.env` z ustawieniami, ktÃ³re moÅ¼esz edytowaÄ‡:

```
# Konfiguracja modelu Ollama
MODEL_NAME="tinyllama:latest"

# Konfiguracja serwera
OLLAMA_URL="http://localhost:11434"
SERVER_PORT=5001

# Parametry generowania
TEMPERATURE=0.7
MAX_TOKENS=1000
```

## ObsÅ‚ugiwane modele

Skrypt dziaÅ‚a z wszystkimi modelami dostÄ™pnymi w Ollama. Oto szczegÃ³Å‚owa lista najpopularniejszych modeli:

| Model | Rozmiar | Przeznaczenie |
|-------|---------|---------------|
| **llama3** | 8B | OgÃ³lnego przeznaczenia, dobry do wiÄ™kszoÅ›ci zadaÅ„ |
| **phi3** | 3.8B | Szybki, dobry do prostszych zadaÅ„, zoptymalizowany pod kÄ…tem kodu |
| **mistral** | 7B | OgÃ³lnego przeznaczenia, efektywny energetycznie |
| **gemma** | 7B | Dobry do zadaÅ„ jÄ™zyka naturalnego i kreatywnego pisania |
| **tinyllama** | 1.1B | Bardzo szybki, idealny dla sÅ‚abszych urzÄ…dzeÅ„ |
| **qwen** | 7-14B | Dobry w analizie tekstu, wsparcie dla jÄ™zykÃ³w azjatyckich |
| **llava** | 7-13B | Multimodalny z obsÅ‚ugÄ… obrazÃ³w - pozwala na analizÄ™ obrazÃ³w i tekstu |
| **codellama** | 7-34B | Wyspecjalizowany model do kodowania - Å›wietny do generowania i analizy kodu |
| **vicuna** | 7-13B | Wytrenowany na konwersacjach, dobry do dialogÃ³w |
| **falcon** | 7-40B | Szybki i efektywny, dobry stosunek wydajnoÅ›ci do rozmiaru |
| **orca-mini** | 3B | Dobry do podstawowych zadaÅ„ NLP |
| **wizardcoder** | 13B | Stworzony do zadaÅ„ zwiÄ…zanych z kodem |
| **llama2** | 7-70B | Poprzednia generacja modelu Meta, sprawdzony w rÃ³Å¼nych zastosowaniach |
| **stablelm** | 3-7B | Dobry do generowania tekstu i dialogÃ³w |
| **dolphin** | 7B | Koncentruje siÄ™ na naturalnoÅ›ci dialogÃ³w |
| **neural-chat** | 7-13B | Zoptymalizowany pod kÄ…tem urzÄ…dzeÅ„ Intel |
| **starling** | 7B | Mniejszy ale skuteczny, zoptymalizowany pod kÄ…tem jakoÅ›ci odpowiedzi |
| **openhermes** | 7B | Dobra dokÅ‚adnoÅ›Ä‡, postÄ™powanie zgodnie z instrukcjami |
| **yi** | 6-34B | Zaawansowany model wielojÄ™zyczny |

### WybÃ³r rozmiaru modelu

Przy wyborze wÅ‚asnego modelu, warto rozwaÅ¼yÄ‡ rÃ³Å¼ne rozmiary:

- **Mini (1-3B)**: Bardzo maÅ‚e modele - szybkie, ale ograniczone moÅ¼liwoÅ›ci
- **Small (3-7B)**: MaÅ‚e modele - dobry kompromis szybkoÅ›Ä‡/jakoÅ›Ä‡
- **Medium (8-13B)**: Åšrednie modele - lepsze odpowiedzi, wymaga wiÄ™cej RAM
- **Large (30-70B)**: DuÅ¼e modele - najlepsza jakoÅ›Ä‡, wysokie wymagania sprzÄ™towe

### Wymagania sprzÄ™towe

PoniÅ¼ej orientacyjne wymagania sprzÄ™towe dla rÃ³Å¼nych rozmiarÃ³w modeli:

| Rozmiar modelu | Minimalna iloÅ›Ä‡ RAM | Zalecana iloÅ›Ä‡ RAM | GPU |
|----------------|---------------------|-------------------|-----|
| Mini (1-3B) | 4GB | 8GB | Opcjonalnie |
| Small (3-7B) | 8GB | 16GB | Zalecany |
| Medium (8-13B) | 16GB | 24GB | Zalecany â‰¥4GB VRAM |
| Large (30-70B) | 32GB | 64GB | Wymagany â‰¥8GB VRAM |

## RozwiÄ…zywanie problemÃ³w

### Problem: BÅ‚Ä…d instalacji pakietÃ³w

Skrypt prÃ³buje instalowaÄ‡ pakiety na kilka sposobÃ³w. W przypadku bÅ‚Ä™dÃ³w, sprÃ³buj manualnie:

```bash
pip install flask requests python-dotenv
```

### Problem: Nie moÅ¼na poÅ‚Ä…czyÄ‡ z Ollama

Upewnij siÄ™, Å¼e serwer Ollama jest uruchomiony:

```bash
ollama serve
```

### Problem: Model nie jest dostÄ™pny

Pobierz model za pomocÄ…:

```bash
ollama pull tinyllama
```

### Problem: Skrypt nie dziaÅ‚a w Å›rodowisku wirtualnym

W Å›rodowiskach wirtualnych (virtualenv, venv) instalacja z flagÄ… `--user` moÅ¼e powodowaÄ‡ problemy. Skrypt powinien automatycznie wykryÄ‡ ten problem i zainstalowaÄ‡ pakiety bez tej flagi.

## PorÃ³wnanie z innymi rozwiÄ…zaniami

Nasz uniwersalny skrypt Å‚Ä…czy zalety wszystkich wczeÅ›niejszych implementacji:

| Funkcja |  server.py | Model Context Protocol |  skrypt |
|---------|-------------------|------------------------|-------------|
| ZaleÅ¼noÅ›ci | Minimalne | Skomplikowane | Automatyczna instalacja |
| Wsparcie dla systemÃ³w | Ograniczone | Zmienne | Wszystkie dystrybucje Linux + macOS |
| Interfejs web | Podstawowy | Brak | Zaawansowany |
| Konfiguracja modeli | RÄ™czna | Skomplikowana | Interaktywny wybÃ³r |
| KompatybilnoÅ›Ä‡ z Ollama | Podstawowa | Åšrednia | PeÅ‚na |
| OdpornoÅ›Ä‡ na bÅ‚Ä™dy | Niska | Åšrednia | Wysoka |

## Rozwijanie projektu

Ten skrypt moÅ¼e byÄ‡ Å‚atwo rozszerzony o dodatkowe funkcje:

1. Dodanie wsparcia dla strumieniowania odpowiedzi
2. Implementacja historii konwersacji
3. Dodanie interfejsu graficznego
4. Rozszerzenie o inne backendowe LLM (np. LocalAI)

## Kontakt i pomoc

JeÅ›li napotkasz problemy, zobacz peÅ‚ne logi skryptu lub uruchom go z opcjÄ… `--check` aby zdiagnozowaÄ‡ problemy z konfiguracjÄ….

---


Prosty, Å‚atwy w uÅ¼yciu serwer i klient Model Context Protocol (MCP) integrujÄ…cy siÄ™ z wybranym modelem, np. TinyLLM, qwen, przez Ollama.

## Logs

```bash
bash setup.sh
```

```bash
========================================================
   Uniwersalne Å›rodowisko dla Ollama API   
========================================================
[INFO] Konfiguracja Å›rodowiska...
[INFO] Sprawdzanie wymagaÅ„ systemowych...
[INFO] Sprawdzanie instalacji Pythona...
[OK] Python 3 znaleziony (Python 3.13.3)
[INFO] Sprawdzanie instalacji pip...
[OK] pip znaleziony (pip 25.1.1 from /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages/pip (python 3.13))
[INFO] Sprawdzanie instalacji Ollama...
[OK] Ollama znaleziona
[INFO] Sprawdzanie pliku server.py...
[OK] Plik server.py znaleziony
[OK] Wszystkie wymagania systemowe sÄ… speÅ‚nione
[INFO] Instalacja wymaganych pakietÃ³w Python...
[INFO] Instalacja pakietÃ³w: flask requests python-dotenv
[UWAGA] PrÃ³ba instalacji bez opcji --user...
Requirement already satisfied: flask in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (3.1.1)
Requirement already satisfied: requests in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (2.32.3)
Requirement already satisfied: python-dotenv in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (1.1.0)
Requirement already satisfied: blinker>=1.9.0 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (1.9.0)
Requirement already satisfied: click>=8.1.3 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (8.2.0)
Requirement already satisfied: itsdangerous>=2.2.0 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (2.2.0)
Requirement already satisfied: jinja2>=3.1.2 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (3.1.6)
Requirement already satisfied: markupsafe>=2.1.1 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (3.0.2)
Requirement already satisfied: werkzeug>=3.1.0 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from flask) (3.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from requests) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from requests) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from requests) (2.4.0)
Requirement already satisfied: certifi>=2017.4.17 in /home/tom/github/fin-officer/mcps/.venv/lib64/python3.13/site-packages (from requests) (2025.4.26)
[INFO] Weryfikacja instalacji pakietÃ³w...
[OK] Wszystkie pakiety zostaÅ‚y zainstalowane pomyÅ›lnie
[INFO] Aktualizacja pliku konfiguracyjnego .env...
[OK] Plik .env zostaÅ‚ zaktualizowany
[OK] Åšrodowisko zostaÅ‚o pomyÅ›lnie skonfigurowane
[OK] Serwer Ollama juÅ¼ dziaÅ‚a
Czy chcesz skonfigurowaÄ‡ modele Ollama? (t/n): t
[INFO] Konfiguracja modeli Ollama...
[INFO] Pobieranie listy dostÄ™pnych modeli...
DostÄ™pne modele:
 1) qwen:latest
 2) phi:latest
 3) tinyllama:latest

Czy chcesz pobraÄ‡ nowy model? (t/n): t
Popularne modele:
1) llama3 - Najnowszy model Meta (8B parametrÃ³w)
2) phi3 - Model Microsoft (3.8B parametrÃ³w, szybki)
3) mistral - Dobry kompromis jakoÅ›Ä‡/rozmiar (7B parametrÃ³w)
4) gemma - Model Google (7B parametrÃ³w)
5) tinyllama - MaÅ‚y model (1.1B parametrÃ³w, bardzo szybki)
6) inny - WÅ‚asny wybÃ³r modelu
Wybierz model do pobrania (1-6): 2
[INFO] Pobieranie modelu phi3...
pulling manifest 
pulling 633fc5be925f: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 2.2 GB                         
pulling fa8235e5b48f: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 1.1 KB                         
pulling 542b217f179c: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  148 B                         
pulling 8dde1baf1db0: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   78 B                         
pulling 23291dc44752: 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  483 B                         
verifying sha256 digest 
writing manifest 
success 
[OK] Model phi3 zostaÅ‚ pobrany pomyÅ›lnie
[INFO] Uruchamianie serwera API (server.py)...
[OK] Serwer bÄ™dzie dostÄ™pny pod adresem: http://localhost:5001
[UWAGA] NaciÅ›nij Ctrl+C aby zatrzymaÄ‡ serwer
ğŸš€ Uruchamianie zaawansowanego serwera Ollama...
ğŸ“„ Konfiguracja z pliku .env:
  - Model: qwen:latest
  - URL Ollama: http://localhost:11434
  - Port serwera: 5001
  - Temperatura: 0.7
  - Max tokenÃ³w: 1000
âœ… Ollama dziaÅ‚a poprawnie, model qwen:latest jest dostÄ™pny
ğŸ”Œ Uruchamianie serwera na porcie 5001...
â„¹ï¸ DostÄ™pne endpointy: /, /ask, /models, /echo
ğŸ“ Interfejs web: http://localhost:5001
 * Serving Flask app 'server'
 * Debug mode: on
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5001
 * Running on http://192.168.188.101:5001
Press CTRL+C to quit
 * Restarting with stat
ğŸš€ Uruchamianie zaawansowanego serwera Ollama...
ğŸ“„ Konfiguracja z pliku .env:
  - Model: qwen:latest
  - URL Ollama: http://localhost:11434
  - Port serwera: 5001
  - Temperatura: 0.7
  - Max tokenÃ³w: 1000
âœ… Ollama dziaÅ‚a poprawnie, model qwen:latest jest dostÄ™pny
ğŸ”Œ Uruchamianie serwera na porcie 5001...
â„¹ï¸ DostÄ™pne endpointy: /, /ask, /models, /echo
ğŸ“ Interfejs web: http://localhost:5001
 * Debugger is active!
 * Debugger PIN: 835-567-853
 * Detected change in '/home/tom/github/fin-officer/mcps/3/config.py', reloading
 * Restarting with stat
ğŸš€ Uruchamianie zaawansowanego serwera Ollama...
ğŸ“„ Konfiguracja z pliku .env:
  - Model: qwen:latest
  - URL Ollama: http://localhost:11434
  - Port serwera: 5001
  - Temperatura: 0.7
  - Max tokenÃ³w: 1000
âœ… Ollama dziaÅ‚a poprawnie, model qwen:latest jest dostÄ™pny
ğŸ”Œ Uruchamianie serwera na porcie 5001...
â„¹ï¸ DostÄ™pne endpointy: /, /ask, /models, /echo
ğŸ“ Interfejs web: http://localhost:5001
 * Debugger is active!
 * Debugger PIN: 835-567-853
127.0.0.1 - - [21/May/2025 10:08:25] "GET / HTTP/1.1" 200 -

===== NOWE ZAPYTANIE =====
Content-Type: application/json
DÅ‚ugoÅ›Ä‡ danych: 66 bajtÃ³w
Surowe dane: {"prompt":"Co to jest python","temperature":0.7,"max_tokens":1000}
ğŸ“¤ Zapytanie: Co to jest python...
ğŸ“¤ WysyÅ‚anie zapytania do Ollama: {"model": "qwen:latest", "prompt": "Co to jest python", "temperature": 0.7, "max_tokens": 1000, "stream": false}...
ğŸ“¥ OdpowiedÅº Ollama (dÅ‚ugoÅ›Ä‡: 289 znakÃ³w)
127.0.0.1 - - [21/May/2025 10:08:45] "POST /ask HTTP/1.1" 200 -

===== NOWE ZAPYTANIE =====
Content-Type: application/json
DÅ‚ugoÅ›Ä‡ danych: 73 bajtÃ³w
Surowe dane: {"prompt":"Kim jest Andrzej Duda?\n","temperature":0.7,"max_tokens":1000}
ğŸ“¤ Zapytanie: Kim jest Andrzej Duda?
...
ğŸ“¤ WysyÅ‚anie zapytania do Ollama: {"model": "qwen:latest", "prompt": "Kim jest Andrzej Duda?\n", "temperature": 0.7, "max_tokens": 1000, "stream": false}...
ğŸ“¥ OdpowiedÅº Ollama (dÅ‚ugoÅ›Ä‡: 155 znakÃ³w)
127.0.0.1 - - [21/May/2025 10:09:57] "POST /ask HTTP/1.1" 200 -
```

Skrypt:
- Sprawdzi i zainstaluje wymagane pakiety
- Sprawdzi czy Ollama jest zainstalowana i uruchomiona
- Pobierze model TinyLLM jeÅ›li nie jest dostÄ™pny
- Uruchomi serwer MCP

### 2. Zadawanie pytaÅ„ do modelu

W nowym terminalu:

```bash
./ask.sh "Co to jest Python?"
```

Lub uÅ¼yj `curl` bezpoÅ›rednio:

```bash
curl -X POST -H "Content-Type: application/json" \
     -d '{"name":"ask_tinyllm","arguments":{"prompt":"Co to jest Python?"}}' \
     http://localhost:8000/v1/tools
```

## Jak to dziaÅ‚a?

### Architektura

```
[Klient] --> [Serwer MCP] --> [Ollama API] --> [TinyLLM]
```

### Komponenty

1. **minimal_mcp_ollama.py** - serwer MCP udostÄ™pniajÄ…cy dwa narzÄ™dzia:
   - `ask_tinyllm` - przekazuje zapytanie do modelu TinyLLM
   - `echo` - proste narzÄ™dzie do testowania

2. **minimal_mcp_client.py** - minimalistyczny klient do komunikacji z serwerem

3. **server.sh** - skrypt konfiguracyjny i startowy

4. **ask_tinyllm.sh** - wygodny alias do korzystania z klienta

## Korzystanie z API

### API MCP (port 8000)

Serwer udostÄ™pnia narzÄ™dzia przez standardowe API MCP:

```
POST http://localhost:8000/v1/tools
```

Format zapytania:
```json
{
  "name": "ask_tinyllm",
  "arguments": {
    "prompt": "Twoje pytanie tutaj"
  }
}
```

### BezpoÅ›rednie API Ollama (port 11434)

MoÅ¼esz rÃ³wnieÅ¼ komunikowaÄ‡ siÄ™ z Ollama bezpoÅ›rednio:

```bash
curl -X POST http://localhost:11434/api/generate \
     -d '{"model": "tinyllama", "prompt": "Co to jest Python?"}'
```

## Co dalej?

Ten minimalistyczny przykÅ‚ad moÅ¼na Å‚atwo rozszerzyÄ‡:

1. Dodaj wiÄ™cej narzÄ™dzi do serwera MCP
2. Zintegruj z innymi modelami Ollama
3. Dodaj obsÅ‚ugÄ™ parametrÃ³w modelu (temperatura, dÅ‚ugoÅ›Ä‡ odpowiedzi)
4. Dodaj obsÅ‚ugÄ™ komunikacji strumieniowej (streaming)

## RozwiÄ…zywanie problemÃ³w

### Problem: Nie moÅ¼na poÅ‚Ä…czyÄ‡ z serwerem Ollama

**RozwiÄ…zanie**: Uruchom serwer Ollama w nowym terminalu:
```bash
ollama serve
```

### Problem: Model tinyllama nie jest dostÄ™pny

**RozwiÄ…zanie**: Pobierz model:
```bash
ollama pull tinyllama
```

### Problem: BÅ‚Ä™dy MCP

**RozwiÄ…zanie**: SprawdÅº czy masz zainstalowane wymagane pakiety:
```bash
pip install mcp requests
```

## PorÃ³wnanie z innymi rozwiÄ…zaniami

1. **Oficjalne SDK MCP**:
   - Zalety: WiÄ™cej funkcji, lepsza dokumentacja
   - Wady: Bardziej zÅ‚oÅ¼one, wiÄ™cej zaleÅ¼noÅ›ci, problemy z kompatybilnoÅ›ciÄ…

2. **RozwiÄ…zanie minimalistyczne (to)**:
   - Zalety: Proste, minimalne zaleÅ¼noÅ›ci, Å‚atwe do zrozumienia i modyfikacji
   - Wady: Mniej funkcji, podstawowa implementacja





# Proste rozwiÄ…zanie dla TinyLLM - bez MCP


1. **server.py** - prosty serwer HTTP z endpointami do komunikacji z TinyLLM
2. **client2.py** - klient do komunikacji z serwerem
3. **setup.sh** - skrypt uruchamiajÄ…cy caÅ‚e rozwiÄ…zanie

## Jak to uruchomiÄ‡?

1. Zapisz wszystkie trzy pliki w tym samym katalogu
2. Nadaj uprawnienia do wykonania skryptom:
   ```bash
   chmod +x setup.sh server.py client.py ask.sh
   ```
3. Uruchom skrypt startowy:
   ```bash
   ./server.sh
   ```

## Jak korzystaÄ‡ z rozwiÄ…zania?

Po uruchomieniu serwera, moÅ¼esz zadawaÄ‡ pytania do TinyLLM na kilka sposobÃ³w:

### 1. UÅ¼yj skryptu klienta

W nowym terminalu:
```bash
./ask.sh "Co to jest Python?"
```

### 2. UÅ¼yj curl

```bash
curl -X POST -H "Content-Type: application/json" -d '{"prompt":"Co to jest Python?"}' http://localhost:5000/ask
```

### 3. PrzeglÄ…darka internetowa

OtwÃ³rz `http://localhost:5000` w przeglÄ…darce, aby zobaczyÄ‡ interfejs z instrukcjami.

## Zalety tego rozwiÄ…zania

1. **Minimalne zaleÅ¼noÅ›ci** - potrzebujesz tylko Flask i requests, bez MCP
2. **Proste API** - endpointy `/ask` i `/echo` z komunikacjÄ… JSON
3. **NiezawodnoÅ›Ä‡** - eliminuje problemy z kompatybilnoÅ›ciÄ… MCP
4. **Lepsze debugowanie** - jasne komunikaty bÅ‚Ä™dÃ³w
5. **Interfejs web** - dostÄ™pny pod adresem http://localhost:5000
6. **ObsÅ‚uga bÅ‚Ä™dÃ³w** - sprawdzanie dostÄ™pnoÅ›ci Ollama i modelu

## Diagnostyka problemÃ³w

JeÅ›li nadal napotykasz problemy:

1. **Czy Ollama jest uruchomiona?** SprawdÅº czy serwer Ollama dziaÅ‚a, uruchamiajÄ…c:
   ```bash
   curl http://localhost:11434/api/tags
   ```

2. **Czy model TinyLLM jest dostÄ™pny?** SprawdÅº listÄ™ modeli:
   ```bash
   ollama list
   ```

3. **Test poÅ‚Ä…czenia z serwerem**:
   ```bash
   ./client.py --test
   ```

## PorÃ³wnanie z MCP

| Aspekt | MCP | Super proste rozwiÄ…zanie |
|--------|-----|-------------------------|
| ZaleÅ¼noÅ›ci | Wymaga MCP SDK, wiele warstw | Tylko Flask i requests |
| StabilnoÅ›Ä‡ | Problemy z kompatybilnoÅ›ciÄ… | Proste, stabilne API |
| Debugowanie | Skomplikowane bÅ‚Ä™dy | Czytelne komunikaty |
| Interfejs | Brak | Prosty interfejs web |
| Konfiguracja | ZÅ‚oÅ¼ona | Minimalna |

To rozwiÄ…zanie moÅ¼e sÅ‚uÅ¼yÄ‡ jako prosty punkt startowy, ktÃ³ry moÅ¼esz pÃ³Åºniej rozbudowaÄ‡ o bardziej zaawansowane funkcje, gdy podstawowa integracja bÄ™dzie juÅ¼ dziaÅ‚aÄ‡.

# Serwer Ollama z Wyborem Modeli

Kompletne rozwiÄ…zanie do uruchamiania i zarzÄ…dzania lokalnym serwerem modeli jÄ™zykowych poprzez Ollama.

## Funkcje

- ğŸš€ Åatwa konfiguracja i zarzÄ…dzanie modelami Ollama
- ğŸ“ Interfejs web do testowania modeli
- âš™ï¸ Konfiguracja przez plik .env
- ğŸŒ API REST do integracji z aplikacjami
- ğŸ”„ ObsÅ‚uga wielu modeli z moÅ¼liwoÅ›ciÄ… przeÅ‚Ä…czania
- ğŸ“Š Zaawansowane zarzÄ…dzanie parametrami generowania

## Wymagania

- Python 3.8+
- Flask i requests (`pip install flask requests python-dotenv`)
- [Ollama](https://ollama.com/download)
- Co najmniej jeden model Ollama (np. tinyllama, llama2, phi)

## Instalacja

1. Sklonuj to repozytorium lub pobierz pliki
2. Zainstaluj wymagane pakiety:
   ```bash
   pip install flask requests python-dotenv
   ```
3. Zainstaluj i uruchom Ollama:
   ```bash
   # Instalacja ze strony https://ollama.com/download
   ollama serve  # Uruchom serwer Ollama
   ```

## Szybki start

1. **Skonfiguruj model**:
   ```bash
   ./model.sh
   ```
   
   Skrypt pomoÅ¼e Ci wybraÄ‡ i pobraÄ‡ model, a nastÄ™pnie skonfigurowaÄ‡ plik .env.

2. **Uruchom serwer**:
   ```bash
   python server.py
   ```
   
   Serwer uruchomi siÄ™ domyÅ›lnie na porcie 5001.

3. **UÅ¼ywaj interfejsu web**:
   
   OtwÃ³rz przeglÄ…darkÄ™ i przejdÅº do: http://localhost:5001

4. **Lub korzystaj z API bezpoÅ›rednio**:
   ```bash
   ./ask.sh "Co to jest Python?"
   ```

## Struktura projektu

- `model.sh` - Skrypt do konfiguracji i pobierania modeli Ollama
- `server.py` - GÅ‚Ã³wny serwer Flask z interfejsem web i API REST
- `env_loader.py` - ModuÅ‚ do Å‚adowania konfiguracji z pliku .env
- `ask.sh` - Skrypt klienta do zadawania pytaÅ„
- `.env` - Plik konfiguracyjny (tworzony automatycznie)

## Plik .env

Plik `.env` zawiera konfiguracjÄ™ serwera i jest tworzony automatycznie przez skrypt `model.sh` lub moduÅ‚ `env_loader.py`. PrzykÅ‚adowa zawartoÅ›Ä‡:

```
# Konfiguracja modelu Ollama
MODEL_NAME="tinyllama:latest"

# Konfiguracja serwera
OLLAMA_URL="http://localhost:11434"
SERVER_PORT=5001

# Parametry generowania
TEMPERATURE=0.7
MAX_TOKENS=1000
```

## API REST

Serwer udostÄ™pnia nastÄ™pujÄ…ce endpointy:

### `POST /ask`

WysyÅ‚a zapytanie do modelu Ollama.

**Å»Ä…danie**:
```json
{
  "prompt": "Co to jest Python?",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

**OdpowiedÅº**:
```json
{
  "response": "Python to wysokopoziomowy, interpretowany jÄ™zyk programowania..."
}
```

### `GET /models`

Pobiera listÄ™ dostÄ™pnych modeli Ollama.

**OdpowiedÅº**:
```json
{
  "models": [
    {
      "name": "tinyllama:latest",
      "size": 1640,
      "current": true
    },
    {
      "name": "llama2:latest",
      "size": 3827,
      "current": false
    }
  ]
}
```

### `POST /echo`

Proste narzÄ™dzie do testowania dziaÅ‚ania serwera.

**Å»Ä…danie**:
```json
{
  "message": "Test"
}
```

**OdpowiedÅº**:
```json
{
  "response": "Otrzymano: Test"
}
```

## PrzykÅ‚ady uÅ¼ycia

### Z cURL

```bash
# Zapytanie do modelu
curl -X POST -H "Content-Type: application/json" \
     -d '{"prompt":"Co to jest Python?"}' \
     http://localhost:5001/ask

# Pobranie listy modeli
curl http://localhost:5001/models

# Test echo
curl -X POST -H "Content-Type: application/json" \
     -d '{"message":"Test"}' \
     http://localhost:5001/echo
```

### Z Python

```python
import requests

# Zapytanie do modelu
response = requests.post(
    "http://localhost:5001/ask",
    json={"prompt": "Co to jest Python?"}
)
print(response.json()["response"])

# Pobranie listy modeli
models = requests.get("http://localhost:5001/models").json()["models"]
print(models)
```

## Zaawansowane uÅ¼ycie

### Zmiana modelu

MoÅ¼esz zmieniÄ‡ model na dwa sposoby:

1. **Ponowna konfiguracja**:
   ```bash
   ./model.sh
   ```

2. **Aktualizacja pliku .env**:
   Edytuj plik `.env` i zmieÅ„ wartoÅ›Ä‡ `MODEL_NAME`, a nastÄ™pnie zrestartuj serwer.

3. **Aktualizacja przez Python**:
   ```bash
   python env_loader.py "gemma:latest"
   ```

### BezpoÅ›rednie zapytanie do Ollama

MoÅ¼esz pominÄ…Ä‡ serwer Flask i wysyÅ‚aÄ‡ zapytania bezpoÅ›rednio do Ollama:

```bash
./ask.sh "Czym jest sztuczna inteligencja?" --direct
```

## RozwiÄ…zywanie problemÃ³w

### Nie moÅ¼na poÅ‚Ä…czyÄ‡ z Ollama

Upewnij siÄ™, Å¼e serwer Ollama jest uruchomiony:

```bash
ollama serve
```

### Model nie jest dostÄ™pny

Pobierz model za pomocÄ…:

```bash
ollama pull tinyllama
```

### BÅ‚Ä…d parsowania JSON

JeÅ›li otrzymujesz bÅ‚Ä…d parsowania JSON, sprÃ³buj zaktualizowaÄ‡ skrypt `ask.sh` lub uÅ¼yj interfejsu web.


