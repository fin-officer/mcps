#!/usr/bin/env python3
"""
Naprawiony serwer dla TinyLLM via Ollama.
Obs≈Çuguje problemy z parsowaniem JSON z Ollama.
"""

import os
import sys
import json
import traceback
import requests
from flask import Flask, request, jsonify

# Konfiguracja
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "tinyllama:latest"  # U≈ºycie pe≈Çnej nazwy z tagiem
PORT = 5001

app = Flask(__name__)


def check_ollama_available():
    """Sprawdza, czy Ollama jest dostƒôpna i model za≈Çadowany."""
    try:
        # Sprawdzenie, czy serwer Ollama dzia≈Ça
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è B≈ÇƒÖd: Serwer Ollama nie odpowiada poprawnie. Kod: {response.status_code}")
            return False

        # Sprawdzenie, czy wymagany model jest dostƒôpny
        models = response.json().get("models", [])
        model_exists = False
        for model in models:
            if model["name"] == MODEL_NAME or model["name"].startswith(MODEL_NAME.split(':')[0]):
                model_exists = True
                break

        if not model_exists:
            print(f"‚ö†Ô∏è Model {MODEL_NAME} nie jest dostƒôpny. Dostƒôpne modele:")
            for model in models:
                print(f" - {model['name']}")
            print(f"\nZainstaluj model komendƒÖ: ollama pull {MODEL_NAME}")
            return False

        print(f"‚úÖ Ollama dzia≈Ça poprawnie, model {MODEL_NAME} jest dostƒôpny")
        return True
    except requests.exceptions.ConnectionError:
        print("‚ö†Ô∏è B≈ÇƒÖd: Nie mo≈ºna po≈ÇƒÖczyƒá siƒô z serwerem Ollama (port 11434)")
        print("‚ÑπÔ∏è Uruchom Ollama komendƒÖ: ollama serve")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è B≈ÇƒÖd: {str(e)}")
        return False


@app.route('/', methods=['GET'])
def home():
    """Strona g≈Ç√≥wna z instrukcjami."""
    return """
    <html>
    <head>
        <title>Super prosty serwer TinyLLM</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            pre { background: #f4f4f4; padding: 10px; border-radius: 5px; }
            .endpoint { font-weight: bold; color: #4CAF50; }
            .warning { color: #f44336; }
        </style>
    </head>
    <body>
        <h1>Super prosty serwer TinyLLM</h1>
        <p>Dostƒôpne endpointy:</p>

        <h2 class="endpoint">POST /ask</h2>
        <p>Zadaj pytanie do modelu TinyLLM.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"prompt":"Co to jest Python?"}' \\
     http://localhost:5001/ask
        </pre>

        <p class="warning">UWAGA: Zwr√≥ƒá uwagƒô na format JSON - musi byƒá w jednej linii bez bia≈Çych znak√≥w!</p>

        <h2 class="endpoint">POST /echo</h2>
        <p>Proste narzƒôdzie do test√≥w - odbija wiadomo≈õƒá.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"message":"Test"}' \\
     http://localhost:5001/echo
        </pre>

        <h2>Status serwera Ollama</h2>
        <p>Model: <strong>tinyllama:latest</strong></p>
        <p>URL Ollama: <strong>http://localhost:11434</strong></p>

        <h2>≈Åatwiejsze zapytania</h2>
        <p>Mo≈ºesz u≈ºyƒá skryptu pomocniczego do zapyta≈Ñ:</p>
        <pre>
./ask.sh "Twoje pytanie"
        </pre>
    </body>
    </html>
    """


def call_ollama_api(prompt, temperature=0.7, max_tokens=1000):
    """
    Bezpieczne wywo≈Çanie API Ollama z obs≈ÇugƒÖ b≈Çƒôd√≥w.

    Args:
        prompt: Zapytanie do modelu
        temperature: Temperatura generowania
        max_tokens: Maksymalna liczba token√≥w w odpowiedzi

    Returns:
        str: Odpowied≈∫ od modelu lub komunikat o b≈Çƒôdzie
    """
    try:
        # U≈ºycie stream=False, aby uniknƒÖƒá problem√≥w z parsowaniem JSON
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }

        print(f"üì§ Wysy≈Çanie zapytania do Ollama: {json.dumps(payload)[:200]}...")

        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json=payload,
            timeout=60
        )

        # Sprawdzenie statusu odpowiedzi
        if response.status_code != 200:
            error_msg = f"B≈ÇƒÖd Ollama: Kod {response.status_code}"
            print(f"‚ö†Ô∏è {error_msg}")
            return None, error_msg

        # Pr√≥ba parsowania odpowiedzi jako JSON
        try:
            # Tylko pierwszy wiersz tre≈õci odpowiedzi
            first_line = response.text.strip().split('\n')[0]
            result = json.loads(first_line)
            ollama_response = result.get("response", "")
            print(f"üì• Odpowied≈∫ Ollama (d≈Çugo≈õƒá: {len(ollama_response)} znak√≥w)")
            return ollama_response, None
        except json.JSONDecodeError as e:
            # Je≈õli parsowanie nie powiod≈Ço siƒô, wy≈õwietl surowe dane
            print(f"‚ö†Ô∏è B≈ÇƒÖd parsowania JSON: {str(e)}")
            print(f"Surowa odpowied≈∫: {response.text[:200]}...")

            # Spr√≥buj wyciƒÖgnƒÖƒá tekst odpowiedzi bez polegania na JSON
            try:
                # Rƒôczne parsowanie dla strumieniowej odpowiedzi
                text_content = ""
                for line in response.text.strip().split('\n'):
                    try:
                        chunk = json.loads(line)
                        if "response" in chunk:
                            text_content += chunk["response"]
                    except:
                        pass

                if text_content:
                    print(f"üì• Odpowied≈∫ wyodrƒôbniona rƒôcznie (d≈Çugo≈õƒá: {len(text_content)} znak√≥w)")
                    return text_content, None
            except Exception as parsing_ex:
                print(f"‚ö†Ô∏è Nie mo≈ºna wyodrƒôbniƒá odpowiedzi: {str(parsing_ex)}")

            # Je≈õli wszystko zawiedzie, zwr√≥ƒá b≈ÇƒÖd
            return None, f"B≈ÇƒÖd parsowania odpowiedzi Ollama: {str(e)}"
    except requests.exceptions.Timeout:
        error_msg = "Timeout podczas oczekiwania na odpowied≈∫ z Ollama"
        print(f"‚ö†Ô∏è {error_msg}")
        return None, error_msg
    except Exception as e:
        error_msg = f"Nieoczekiwany b≈ÇƒÖd: {str(e)}"
        print(f"‚ö†Ô∏è {error_msg}")
        traceback.print_exc()
        return None, error_msg


@app.route('/ask', methods=['POST'])
def ask_tinyllm():
    """Zadaj pytanie do modelu TinyLLM poprzez Ollama."""
    # Debugowanie ≈ºƒÖdania
    print(f"\n===== NOWE ZAPYTANIE =====")
    print(f"Content-Type: {request.headers.get('Content-Type', 'brak')}")
    print(f"D≈Çugo≈õƒá danych: {request.content_length} bajt√≥w")
    print(f"Surowe dane: {request.data.decode('utf-8', errors='replace')}")

    # Sprawdzenie Content-Type
    if not request.is_json:
        error_msg = "Oczekiwano danych JSON. U≈ºyj nag≈Ç√≥wka 'Content-Type: application/json'"
        print(f"‚ö†Ô∏è B≈ÇƒÖd: {error_msg}")
        return jsonify({"error": error_msg}), 400

    # Obs≈Çuga b≈Çƒôd√≥w parsowania JSON
    try:
        data = request.get_json(force=False)
    except Exception as e:
        error_msg = f"B≈ÇƒÖd parsowania JSON: {str(e)}"
        print(f"‚ö†Ô∏è {error_msg}")
        print(f"Stacktrace: {traceback.format_exc()}")
        return jsonify({"error": error_msg}), 400

    # Sprawdzenie struktury JSON
    if not isinstance(data, dict):
        error_msg = f"Oczekiwano obiektu JSON, otrzymano: {type(data).__name__}"
        print(f"‚ö†Ô∏è {error_msg}")
        return jsonify({"error": error_msg}), 400

    if 'prompt' not in data:
        error_msg = "Brak parametru 'prompt' w danych JSON"
        print(f"‚ö†Ô∏è {error_msg}")
        return jsonify({"error": error_msg}), 400

    prompt = data['prompt']
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)

    print(f"üì§ Zapytanie: {prompt[:50]}...")

    # Wywo≈Çanie API Ollama
    response, error = call_ollama_api(prompt, temperature, max_tokens)

    if error:
        return jsonify({"error": error}), 500
    else:
        return jsonify({"response": response})


@app.route('/echo', methods=['POST'])
def echo():
    """Proste narzƒôdzie do testowania dzia≈Çania serwera."""
    # Debugowanie ≈ºƒÖdania
    print(f"\n===== NOWE ZAPYTANIE ECHO =====")
    print(f"Content-Type: {request.headers.get('Content-Type', 'brak')}")
    print(f"Surowe dane: {request.data.decode('utf-8', errors='replace')}")

    if not request.is_json:
        error_msg = "Oczekiwano danych JSON. U≈ºyj nag≈Ç√≥wka 'Content-Type: application/json'"
        print(f"‚ö†Ô∏è B≈ÇƒÖd: {error_msg}")
        return jsonify({"error": error_msg}), 400

    try:
        data = request.get_json(force=False)
    except Exception as e:
        error_msg = f"B≈ÇƒÖd parsowania JSON: {str(e)}"
        print(f"‚ö†Ô∏è {error_msg}")
        return jsonify({"error": error_msg}), 400

    if not isinstance(data, dict):
        error_msg = f"Oczekiwano obiektu JSON, otrzymano: {type(data).__name__}"
        print(f"‚ö†Ô∏è {error_msg}")
        return jsonify({"error": error_msg}), 400

    if 'message' not in data:
        error_msg = "Brak parametru 'message' w danych JSON"
        print(f"‚ö†Ô∏è {error_msg}")
        return jsonify({"error": error_msg}), 400

    message = data['message']
    print(f"üì§ Echo: {message}")

    return jsonify({"response": f"Otrzymano: {message}"})


if __name__ == "__main__":
    print("üöÄ Uruchamianie super prostego serwera TinyLLM (z poprawionƒÖ integracjƒÖ Ollama)...")

    # Sprawdzenie dostƒôpno≈õci Ollama przed uruchomieniem
    if not check_ollama_available():
        choice = input("Czy chcesz kontynuowaƒá mimo to? (t/n): ")
        if choice.lower() != 't':
            print("Zamykanie...")
            sys.exit(1)

    # Uruchomienie serwera
    print(f"üîå Uruchamianie serwera na porcie {PORT}...")
    print("‚ÑπÔ∏è Dostƒôpne endpointy: /ask, /echo")

    # Przyk≈Çadowe zapytania:
    print("\nüìù Przyk≈Çady u≈ºycia (zwr√≥ƒá uwagƒô na format JSON - musi byƒá w jednej linii):")
    print(
        f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"message\":\"Test\"}}' http://localhost:{PORT}/echo")
    print(
        f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"prompt\":\"Co to jest Python?\"}}' http://localhost:{PORT}/ask")

    # Uruchomienie serwera na wszystkich interfejsach z lepszym logowaniem
    app.run(host='0.0.0.0', port=PORT, debug=True)