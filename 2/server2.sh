#!/bin/bash

# Super prosty skrypt startowy dla serwera i klienta TinyLLM
# Autor: Tom
# Data: 2025-05-20

# Kolory dla lepszej czytelnoÅ›ci
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}======================================================${NC}"
echo -e "${BLUE}   Super prosty serwer i klient TinyLLM via Ollama    ${NC}"
echo -e "${BLUE}======================================================${NC}"

# Aktywacja Å›rodowiska wirtualnego jeÅ›li istnieje
if [ -d "mcp_env" ]; then
    echo -e "${BLUE}Aktywacja Å›rodowiska wirtualnego...${NC}"
    source mcp_env/bin/activate
fi

# Uruchomienie serwera w tle
echo -e "${GREEN}Uruchamianie serwera TinyLLM...${NC}"
echo -e "${YELLOW}Aby zadaÄ‡ pytanie, uÅ¼yj skryptu:${NC}"
echo -e "${BLUE}./ask_tinyllm_simple.sh \"Twoje pytanie\"${NC}"
echo -e "${YELLOW}Lub w przeglÄ…darce otwÃ³rz:${NC} http://localhost:5000"
echo

# Uruchomienie serwera
python server.py
 Sprawdzenie wymaganych pakietÃ³w Python
required_packages=("flask" "requests")
missing_packages=()

for package in "${required_packages[@]}"; do
    if ! pip show $package &>/dev/null; then
        missing_packages+=("$package")
    fi
done

if [ ${#missing_packages[@]} -gt 0 ]; then
    echo -e "${YELLOW}Instalacja brakujÄ…cych pakietÃ³w: ${missing_packages[*]}${NC}"
    pip install ${missing_packages[*]}
fi

# Sprawdzenie czy Ollama jest zainstalowana
if ! command -v ollama &> /dev/null; then
    echo -e "${RED}UWAGA: Ollama nie jest zainstalowana${NC}"
    echo -e "${YELLOW}Pobierz jÄ… ze strony: https://ollama.com/download${NC}"
else
    echo -e "${GREEN}âœ“ Ollama jest zainstalowana${NC}"

    # Sprawdzenie czy serwer Ollama jest uruchomiony
    if curl -s --head --connect-timeout 2 http://localhost:11434 > /dev/null; then
        echo -e "${GREEN}âœ“ Serwer Ollama jest uruchomiony${NC}"
    else
        echo -e "${RED}âœ— Serwer Ollama nie jest uruchomiony${NC}"
        echo -e "${YELLOW}Uruchamianie serwera Ollama w nowym terminalu...${NC}"

        # PrÃ³ba otwarcia nowego terminala z serwerem Ollama
        gnome-terminal -- bash -c "ollama serve; read -p 'NaciÅ›nij Enter, aby zamknÄ…Ä‡...'" || \
        xterm -e "ollama serve; read -p 'NaciÅ›nij Enter, aby zamknÄ…Ä‡...'" || \
        konsole --new-tab -e "ollama serve; read -p 'NaciÅ›nij Enter, aby zamknÄ…Ä‡...'" || \
        (
            echo -e "${RED}Nie udaÅ‚o siÄ™ uruchomiÄ‡ nowego terminala${NC}"
            echo -e "${YELLOW}Uruchom Ollama rÄ™cznie w nowym terminalu komendÄ…:${NC} ollama serve"
            read -p "NaciÅ›nij Enter, gdy serwer Ollama bÄ™dzie uruchomiony..." dummy
        )
    fi

    # Sprawdzenie czy model tinyllama jest dostÄ™pny
    if ollama list 2>/dev/null | grep -q tinyllama; then
        echo -e "${GREEN}âœ“ Model tinyllama jest dostÄ™pny${NC}"
    else
        echo -e "${RED}âœ— Model tinyllama nie jest dostÄ™pny${NC}"
        echo -e "${YELLOW}Pobieranie modelu tinyllama...${NC}"
        ollama pull tinyllama
    fi
fi

# Utworzenie plikÃ³w serwera i klienta jeÅ›li nie istniejÄ…
if [ ! -f "server.py" ]; then
    echo -e "${BLUE}Tworzenie pliku serwera...${NC}"
    cat > server.py << 'EOL'
#!/usr/bin/env python3
"""
Super prosty serwer HTTP udostÄ™pniajÄ…cy TinyLLM przez Ollama.
Bez zaleÅ¼noÅ›ci od MCP - uÅ¼ywa tylko standardowej biblioteki Flask.
"""

import os
import sys
import json
import requests
from flask import Flask, request, jsonify

# Konfiguracja
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "tinyllama"  # NazwÄ™ modelu moÅ¼na zmieniÄ‡
PORT = 5000

app = Flask(__name__)

def check_ollama_available():
    """Sprawdza, czy Ollama jest dostÄ™pna i model zaÅ‚adowany."""
    try:
        # Sprawdzenie, czy serwer Ollama dziaÅ‚a
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=2)
        if response.status_code != 200:
            print(f"âš ï¸ BÅ‚Ä…d: Serwer Ollama nie odpowiada poprawnie. Kod: {response.status_code}")
            return False

        # Sprawdzenie, czy wymagany model jest dostÄ™pny
        models = response.json().get("models", [])
        if not any(model["name"] == MODEL_NAME for model in models):
            print(f"âš ï¸ Model {MODEL_NAME} nie jest dostÄ™pny. DostÄ™pne modele:")
            for model in models:
                print(f" - {model['name']}")
            print(f"\nZainstaluj model komendÄ…: ollama pull {MODEL_NAME}")
            return False

        print(f"âœ… Ollama dziaÅ‚a poprawnie, model {MODEL_NAME} jest dostÄ™pny")
        return True
    except requests.exceptions.ConnectionError:
        print("âš ï¸ BÅ‚Ä…d: Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z serwerem Ollama (port 11434)")
        print("â„¹ï¸ Uruchom Ollama komendÄ…: ollama serve")
        return False
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d: {str(e)}")
        return False

@app.route('/', methods=['GET'])
def home():
    """Strona gÅ‚Ã³wna z instrukcjami."""
    return """
    <html>
    <head>
        <title>Super prosty serwer TinyLLM</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            pre { background: #f4f4f4; padding: 10px; border-radius: 5px; }
            .endpoint { font-weight: bold; color: #4CAF50; }
        </style>
    </head>
    <body>
        <h1>Super prosty serwer TinyLLM</h1>
        <p>DostÄ™pne endpointy:</p>

        <h2 class="endpoint">POST /ask</h2>
        <p>Zadaj pytanie do modelu TinyLLM.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"prompt": "Co to jest Python?"}' \\
     http://localhost:5000/ask
        </pre>

        <h2 class="endpoint">POST /echo</h2>
        <p>Proste narzÄ™dzie do testÃ³w - odbija wiadomoÅ›Ä‡.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"message": "Test"}' \\
     http://localhost:5000/echo
        </pre>

        <h2>Status serwera Ollama</h2>
        <p>Model: <strong>tinyllama</strong></p>
        <p>URL Ollama: <strong>http://localhost:11434</strong></p>
    </body>
    </html>
    """

@app.route('/ask', methods=['POST'])
def ask_tinyllm():
    """Zadaj pytanie do modelu TinyLLM poprzez Ollama."""
    if not request.is_json:
        return jsonify({"error": "Oczekiwano danych JSON"}), 400

    data = request.get_json()
    if 'prompt' not in data:
        return jsonify({"error": "Brak parametru 'prompt'"}), 400

    prompt = data['prompt']

    print(f"ðŸ“¤ Zapytanie: {prompt[:50]}...")

    try:
        # WywoÅ‚anie API Ollama z minimalnÄ… konfiguracjÄ…
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "temperature": data.get('temperature', 0.7),
                "max_tokens": data.get('max_tokens', 1000)
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            print(f"ðŸ“¥ OdpowiedÅº otrzymana")
            return jsonify({"response": result.get("response", "Brak odpowiedzi od modelu.")})
        else:
            error_msg = f"BÅ‚Ä…d Ollama: {response.status_code}"
            print(f"âš ï¸ {error_msg}")
            return jsonify({"error": error_msg}), 500
    except Exception as e:
        error_msg = f"BÅ‚Ä…d: {str(e)}"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 500

@app.route('/echo', methods=['POST'])
def echo():
    """Proste narzÄ™dzie do testowania dziaÅ‚ania serwera."""
    if not request.is_json:
        return jsonify({"error": "Oczekiwano danych JSON"}), 400

    data = request.get_json()
    if 'message' not in data:
        return jsonify({"error": "Brak parametru 'message'"}), 400

    return jsonify({"response": f"Otrzymano: {data['message']}"})

if __name__ == "__main__":
    print("ðŸš€ Uruchamianie super prostego serwera TinyLLM...")

    # Sprawdzenie dostÄ™pnoÅ›ci Ollama przed uruchomieniem
    if not check_ollama_available():
        choice = input("Czy chcesz kontynuowaÄ‡ mimo to? (t/n): ")
        if choice.lower() != 't':
            print("Zamykanie...")
            sys.exit(1)

    # Uruchomienie serwera
    print(f"ðŸ”Œ Uruchamianie serwera na porcie {PORT}...")
    print("â„¹ï¸ DostÄ™pne endpointy: /ask, /echo")

    # PrzykÅ‚adowe zapytania:
    print("\nðŸ“ PrzykÅ‚ady uÅ¼ycia:")
    print(f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"message\": \"Test\"}}' http://localhost:{PORT}/echo")
    print(f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"prompt\": \"Co to jest Python?\"}}' http://localhost:{PORT}/ask")

    # Uruchomienie serwera na wszystkich interfejsach
    app.run(host='0.0.0.0', port=PORT, debug=False)
EOL
    chmod +x server.py
fi

if [ ! -f "client2.py" ]; then
    echo -e "${BLUE}Tworzenie pliku klienta...${NC}"
    cat > client2.py << 'EOL'
#!/usr/bin/env python3
"""
Super prosty klient do komunikacji z serwerem TinyLLM.
Nie wymaga Å¼adnych specjalnych bibliotek poza requests.
"""

import sys
import requests
import argparse

def send_query(prompt, server_url="http://localhost:5000"):
    """
    WysyÅ‚a zapytanie do serwera TinyLLM.

    Args:
        prompt: Zapytanie do modelu
        server_url: URL serwera

    Returns:
        OdpowiedÅº lub komunikat o bÅ‚Ä™dzie
    """
    print(f"ðŸ“¤ WysyÅ‚anie zapytania: {prompt[:50]}...")

    try:
        # Zapytanie do API serwera
        response = requests.post(
            f"{server_url}/ask",
            json={"prompt": prompt},
            timeout=120  # DÅ‚uÅ¼szy timeout dla modeli jÄ™zykowych
        )

        if response.status_code == 200:
            result = response.json()
            if "response" in result:
                return result["response"]
            elif "error" in result:
                return f"BÅ‚Ä…d: {result['error']}"
            else:
                return "Nieznany format odpowiedzi"
        else:
            return f"BÅ‚Ä…d odpowiedzi: {response.status_code} - {response.text}"
    except requests.exceptions.ConnectionError:
        return "âš ï¸ BÅ‚Ä…d poÅ‚Ä…czenia z serwerem. SprawdÅº czy serwer jest uruchomiony."
    except Exception as e:
        return f"âš ï¸ BÅ‚Ä…d: {str(e)}"

def test_echo(message="Test", server_url="http://localhost:5000"):
    """
    Testuje dziaÅ‚anie serwera uÅ¼ywajÄ…c endpointu echo.

    Args:
        message: WiadomoÅ›Ä‡ testowa
        server_url: URL serwera

    Returns:
        True jeÅ›li test siÄ™ powiÃ³dÅ‚, False w przeciwnym razie
    """
    print(f"ðŸ§ª Testowanie serwera za pomocÄ… echo...")

    try:
        response = requests.post(
            f"{server_url}/echo",
            json={"message": message},
            timeout=5
        )

        if response.status_code == 200:
            result = response.json()
            print(f"âœ… OdpowiedÅº serwera: {result.get('response', '')}")
            return True
        else:
            print(f"âŒ BÅ‚Ä…d odpowiedzi: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d: {str(e)}")
        return False

def main():
    """GÅ‚Ã³wna funkcja klienta."""
    # Parsowanie argumentÃ³w wiersza poleceÅ„
    parser = argparse.ArgumentParser(description="Super prosty klient TinyLLM")
    parser.add_argument("prompt", nargs='?', help="Zapytanie do wysÅ‚ania do modelu")
    parser.add_argument("--server", default="http://localhost:5000", help="URL serwera (domyÅ›lnie: http://localhost:5000)")
    parser.add_argument("--test", action="store_true", help="Testuje poÅ‚Ä…czenie z serwerem")
    args = parser.parse_args()

    # JeÅ›li podano opcjÄ™ --test, wykonaj test echo
    if args.test:
        test_success = test_echo(server_url=args.server)
        if not test_success:
            print("âŒ Test nie powiÃ³dÅ‚ siÄ™. Czy serwer jest uruchomiony?")
            sys.exit(1)
        print("âœ… Test zakoÅ„czony pomyÅ›lnie!")

        # JeÅ›li nie podano zapytania, zakoÅ„cz po teÅ›cie
        if not args.prompt:
            sys.exit(0)

    # SprawdÅº czy podano zapytanie
    if not args.prompt:
        parser.print_help()
        sys.exit(1)

    # WysÅ‚anie zapytania
    response = send_query(args.prompt, args.server)

    # WyÅ›wietlenie odpowiedzi
    print("\nðŸ“¥ OdpowiedÅº:")
    print("=" * 50)
    print(response)
    print("=" * 50)

if __name__ == "__main__":
    main()
EOL
    chmod +x client2.py
fi

# Tworzenie skryptu do uÅ¼ywania klienta
if [ ! -f "ask_tinyllm_simple.sh" ]; then
    echo -e "${BLUE}Tworzenie skryptu klienta...${NC}"
    cat > ask_tinyllm_simple.sh << 'EOL'
#!/bin/bash
# Aktywacja Å›rodowiska wirtualnego jeÅ›li istnieje
if [ -d "mcp_env" ]; then
    source mcp_env/bin/activate
fi

# Uruchomienie klienta z przekazanymi argumentami
python client2.py "$@"
EOL
    chmod +x ask_tinyllm_simple.sh
fi

# Test poÅ‚Ä…czenia z serwerem Ollama
echo -e "${BLUE}Testowanie poÅ‚Ä…czenia z Ollama...${NC}"
if curl -s --head --connect-timeout 2 http://localhost:11434 > /dev/null; then
    echo -e "${GREEN}PoÅ‚Ä…czenie z Ollama dziaÅ‚a poprawnie.${NC}"
else
    echo -e "${RED}Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z Ollama. Czy serwer jest uruchomiony?${NC}"
    read -p "Czy chcesz kontynuowaÄ‡ mimo to? (t/n): " continue_choice
    if [[ "$continue_choice" != "t" ]]; then
        echo "Zatrzymywanie."
        exit 1
    fi
fi

#