#!/usr/bin/env python3
"""
Minimalistyczny serwer MCP z integracjÄ… Ollama TinyLLM
Wszystko w jednym pliku dla maksymalnej prostoty
"""

import os
import sys
import requests
from mcp.server.fastmcp import FastMCP, Context

# Konfiguracja
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "tinyllama"  # NazwÄ™ modelu moÅ¼na zmieniÄ‡


def check_ollama_available():
    """Sprawdza, czy Ollama jest dostÄ™pna i model zaÅ‚adowany."""
    try:
        # Sprawdzenie, czy serwer Ollama dziaÅ‚a
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=2)
        if response.status_code != 200:
            print(f"âš ï¸ BÅ‚Ä…d: Serwer Ollama nie odpowiada poprawnie. Kod: {response.status_code}")
            return False

        # Sprawdzenie, czy wymagany model jest dostÄ™pny
        models = response.json().get("models", [])
        if not any(model["name"] == MODEL_NAME for model in models):
            print(f"âš ï¸ Model {MODEL_NAME} nie jest dostÄ™pny. DostÄ™pne modele:")
            for model in models:
                print(f" - {model['name']}")
            print(f"\nZainstaluj model komendÄ…: ollama pull {MODEL_NAME}")
            return False

        print(f"âœ… Ollama dziaÅ‚a poprawnie, model {MODEL_NAME} jest dostÄ™pny")
        return True
    except requests.exceptions.ConnectionError:
        print("âš ï¸ BÅ‚Ä…d: Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z serwerem Ollama (port 11434)")
        print("â„¹ï¸ Uruchom Ollama komendÄ…: ollama serve")
        return False
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d: {str(e)}")
        return False


# Tworzenie serwera MCP
mcp = FastMCP("MCP-Ollama-TinyLLM")


@mcp.tool()
async def ask_tinyllm(prompt: str, ctx: Context = None) -> str:
    """Zadaj pytanie do modelu TinyLLM poprzez Ollama."""
    if ctx:
        await ctx.info(f"WysyÅ‚anie zapytania do modelu TinyLLM: {prompt[:50]}...")

    try:
        # WywoÅ‚anie API Ollama z minimalnÄ… konfiguracjÄ…
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "temperature": 0.7,
                "max_tokens": 1000
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            return result.get("response", "Brak odpowiedzi od modelu.")
        else:
            error_msg = f"BÅ‚Ä…d Ollama: {response.status_code}"
            if ctx:
                await ctx.error(error_msg)
            return error_msg
    except Exception as e:
        error_msg = f"BÅ‚Ä…d: {str(e)}"
        if ctx:
            await ctx.error(error_msg)
        return error_msg


@mcp.tool()
def echo(message: str) -> str:
    """Proste narzÄ™dzie do testowania dziaÅ‚ania serwera."""
    return f"Otrzymano: {message}"


if __name__ == "__main__":
    print("ğŸš€ Uruchamianie minimalnego serwera MCP z TinyLLM...")

    # Sprawdzenie dostÄ™pnoÅ›ci Ollama przed uruchomieniem
    if not check_ollama_available():
        choice = input("Czy chcesz kontynuowaÄ‡ mimo to? (t/n): ")
        if choice.lower() != 't':
            print("Zamykanie...")
            sys.exit(1)

    # Uruchomienie serwera MCP
    print("ğŸ”Œ Uruchamianie serwera MCP na porcie 8000...")
    print("â„¹ï¸ DostÄ™pne narzÄ™dzia: ask_tinyllm, echo")

    # PrzykÅ‚adowe zapytania:
    print("\nğŸ“ PrzykÅ‚ady uÅ¼ycia:")
    print(
        "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"echo\",\"arguments\":{\"message\":\"Test\"}}' http://localhost:8000/v1/tools")
    print(
        "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"ask_tinyllm\",\"arguments\":{\"prompt\":\"Co to jest Python?\"}}' http://localhost:8000/v1/tools")

    mcp.run()