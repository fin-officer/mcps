#!/usr/bin/env python3
"""
Poprawka do serwera TinyLLM - dodaje lepszÄ… obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w w przetwarzaniu JSON.
Zapisz ten plik, a nastÄ™pnie zastÄ…p nim istniejÄ…cy super_simple_server.py.
"""

import os
import sys
import json
import traceback
import requests
from flask import Flask, request, jsonify

# Konfiguracja
OLLAMA_URL = "http://localhost:11434"
MODEL_NAME = "tinyllama"  # NazwÄ™ modelu moÅ¼na zmieniÄ‡
PORT = 5001

app = Flask(__name__)

def check_ollama_available():
    """Sprawdza, czy Ollama jest dostÄ™pna i model zaÅ‚adowany."""
    try:
        # Sprawdzenie, czy serwer Ollama dziaÅ‚a
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=2)
        if response.status_code != 200:
            print(f"âš ï¸ BÅ‚Ä…d: Serwer Ollama nie odpowiada poprawnie. Kod: {response.status_code}")
            return False

        # Sprawdzenie, czy wymagany model jest dostÄ™pny
        models = response.json().get("models", [])
        if not any(model["name"] == MODEL_NAME for model in models):
            print(f"âš ï¸ Model {MODEL_NAME} nie jest dostÄ™pny. DostÄ™pne modele:")
            for model in models:
                print(f" - {model['name']}")
            print(f"\nZainstaluj model komendÄ…: ollama pull {MODEL_NAME}")
            return False

        print(f"âœ… Ollama dziaÅ‚a poprawnie, model {MODEL_NAME} jest dostÄ™pny")
        return True
    except requests.exceptions.ConnectionError:
        print("âš ï¸ BÅ‚Ä…d: Nie moÅ¼na poÅ‚Ä…czyÄ‡ siÄ™ z serwerem Ollama (port 11434)")
        print("â„¹ï¸ Uruchom Ollama komendÄ…: ollama serve")
        return False
    except Exception as e:
        print(f"âš ï¸ BÅ‚Ä…d: {str(e)}")
        return False

@app.route('/', methods=['GET'])
def home():
    """Strona gÅ‚Ã³wna z instrukcjami."""
    return """
    <html>
    <head>
        <title>Super prosty serwer TinyLLM</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }
            pre { background: #f4f4f4; padding: 10px; border-radius: 5px; }
            .endpoint { font-weight: bold; color: #4CAF50; }
            .warning { color: #f44336; }
        </style>
    </head>
    <body>
        <h1>Super prosty serwer TinyLLM</h1>
        <p>DostÄ™pne endpointy:</p>

        <h2 class="endpoint">POST /ask</h2>
        <p>Zadaj pytanie do modelu TinyLLM.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"prompt":"Co to jest Python?"}' \\
     http://localhost:5001/ask
        </pre>

        <p class="warning">UWAGA: ZwrÃ³Ä‡ uwagÄ™ na format JSON - musi byÄ‡ w jednej linii bez biaÅ‚ych znakÃ³w!</p>

        <h2 class="endpoint">POST /echo</h2>
        <p>Proste narzÄ™dzie do testÃ³w - odbija wiadomoÅ›Ä‡.</p>
        <pre>
curl -X POST -H "Content-Type: application/json" \\
     -d '{"message":"Test"}' \\
     http://localhost:5001/echo
        </pre>

        <h2>Status serwera Ollama</h2>
        <p>Model: <strong>tinyllama</strong></p>
        <p>URL Ollama: <strong>http://localhost:11434</strong></p>

        <h2>Åatwiejsze zapytania</h2>
        <p>MoÅ¼esz uÅ¼yÄ‡ skryptu pomocniczego do zapytaÅ„:</p>
        <pre>
./simple_ask.sh "Twoje pytanie"
        </pre>
    </body>
    </html>
    """

@app.route('/ask', methods=['POST'])
def ask_tinyllm():
    """Zadaj pytanie do modelu TinyLLM poprzez Ollama."""
    # Debugowanie Å¼Ä…dania
    print(f"\n===== NOWE ZAPYTANIE =====")
    print(f"Content-Type: {request.headers.get('Content-Type', 'brak')}")
    print(f"DÅ‚ugoÅ›Ä‡ danych: {request.content_length} bajtÃ³w")
    print(f"Surowe dane: {request.data.decode('utf-8', errors='replace')}")

    # Sprawdzenie Content-Type
    if not request.is_json:
        error_msg = "Oczekiwano danych JSON. UÅ¼yj nagÅ‚Ã³wka 'Content-Type: application/json'"
        print(f"âš ï¸ BÅ‚Ä…d: {error_msg}")
        return jsonify({"error": error_msg}), 400

    # ObsÅ‚uga bÅ‚Ä™dÃ³w parsowania JSON
    try:
        data = request.get_json(force=False)
    except Exception as e:
        error_msg = f"BÅ‚Ä…d parsowania JSON: {str(e)}"
        print(f"âš ï¸ {error_msg}")
        print(f"Stacktrace: {traceback.format_exc()}")
        return jsonify({"error": error_msg}), 400

    # Sprawdzenie struktury JSON
    if not isinstance(data, dict):
        error_msg = f"Oczekiwano obiektu JSON, otrzymano: {type(data).__name__}"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 400

    if 'prompt' not in data:
        error_msg = "Brak parametru 'prompt' w danych JSON"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 400

    prompt = data['prompt']

    print(f"ğŸ“¤ Zapytanie: {prompt[:50]}...")

    try:
        # WywoÅ‚anie API Ollama z minimalnÄ… konfiguracjÄ…
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "temperature": data.get('temperature', 0.7),
                "max_tokens": data.get('max_tokens', 1000)
            },
            timeout=60
        )

        if response.status_code == 200:
            result = response.json()
            print(f"ğŸ“¥ OdpowiedÅº otrzymana")
            return jsonify({"response": result.get("response", "Brak odpowiedzi od modelu.")})
        else:
            error_msg = f"BÅ‚Ä…d Ollama: {response.status_code}"
            print(f"âš ï¸ {error_msg}")
            return jsonify({"error": error_msg}), 500
    except Exception as e:
        error_msg = f"BÅ‚Ä…d: {str(e)}"
        print(f"âš ï¸ {error_msg}")
        print(f"Stacktrace: {traceback.format_exc()}")
        return jsonify({"error": error_msg}), 500

@app.route('/echo', methods=['POST'])
def echo():
    """Proste narzÄ™dzie do testowania dziaÅ‚ania serwera."""
    # Debugowanie Å¼Ä…dania
    print(f"\n===== NOWE ZAPYTANIE ECHO =====")
    print(f"Content-Type: {request.headers.get('Content-Type', 'brak')}")
    print(f"Surowe dane: {request.data.decode('utf-8', errors='replace')}")

    if not request.is_json:
        error_msg = "Oczekiwano danych JSON. UÅ¼yj nagÅ‚Ã³wka 'Content-Type: application/json'"
        print(f"âš ï¸ BÅ‚Ä…d: {error_msg}")
        return jsonify({"error": error_msg}), 400

    try:
        data = request.get_json(force=False)
    except Exception as e:
        error_msg = f"BÅ‚Ä…d parsowania JSON: {str(e)}"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 400

    if not isinstance(data, dict):
        error_msg = f"Oczekiwano obiektu JSON, otrzymano: {type(data).__name__}"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 400

    if 'message' not in data:
        error_msg = "Brak parametru 'message' w danych JSON"
        print(f"âš ï¸ {error_msg}")
        return jsonify({"error": error_msg}), 400

    message = data['message']
    print(f"ğŸ“¤ Echo: {message}")

    return jsonify({"response": f"Otrzymano: {message}"})

if __name__ == "__main__":
    print("ğŸš€ Uruchamianie super prostego serwera TinyLLM (z lepszÄ… obsÅ‚ugÄ… bÅ‚Ä™dÃ³w)...")

    # Sprawdzenie dostÄ™pnoÅ›ci Ollama przed uruchomieniem
    if not check_ollama_available():
        choice = input("Czy chcesz kontynuowaÄ‡ mimo to? (t/n): ")
        if choice.lower() != 't':
            print("Zamykanie...")
            sys.exit(1)

    # Uruchomienie serwera
    print(f"ğŸ”Œ Uruchamianie serwera na porcie {PORT}...")
    print("â„¹ï¸ DostÄ™pne endpointy: /ask, /echo")

    # PrzykÅ‚adowe zapytania:
    print("\nğŸ“ PrzykÅ‚ady uÅ¼ycia (zwrÃ³Ä‡ uwagÄ™ na format JSON - musi byÄ‡ w jednej linii):")
    print(f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"message\":\"Test\"}}' http://localhost:{PORT}/echo")
    print(f"curl -X POST -H \"Content-Type: application/json\" -d '{{\"prompt\":\"Co to jest Python?\"}}' http://localhost:{PORT}/ask")

    # Uruchomienie serwera na wszystkich interfejsach z lepszym logowaniem
    app.run(host='0.0.0.0', port=PORT, debug=True)